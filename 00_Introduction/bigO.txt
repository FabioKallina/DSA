
Introduction to big O:

An algorithm can be seen as a recipe for a computer to follow. It's a set of intructions that a computer will follow step-by=step to solve a problem.

Algorithms take an input and produce an output. The output will be the answer to a question regarding the input. 
For example, let's say you had a non-empty array of positive integers called nums, and you wanted to answer the question: "What is the largest number in nums?".

To answer this question, you would write an algorithm that takes an array called nums as input, and outputs the largest number in nums. 
Here is an example of such an algorithm:

- Create a variable maxNum and initialize it to 0.
- Iterate over each element num in nums.
- If num is greater than maxNum, update maxNum = num.
- Output maxNum.

Here, we have written down a set of instructions that when followed, will solve the problem. We can now implement these instructions in code so that a computer can quickly solve the problem. There are some important requirements for algorithms in the context of LeetCode:

Algorithms should be deterministic. Given the same input, the algorithm should always produce the same output. Basically, there shouldn't be any randomness.
The algorithm should be correct for any arbitrary valid input. In our example, we stated that nums is a non-empty array of positive integers. 
There are infinitely many such arrays, and our algorithm works for all of them. 
However, if nums contained negative numbers, the input would be invalid since we specifically required positive integers. 
In fact, our algorithm would break in such a case because we initialized maxNum to 0. 
If nums were entirely negative, maxNum would remain 0 since no negative number would be greater than 0, leading to an incorrect result. 
We could initialize maxNum to the first element of nums to ensure the maximum value is always selected from the array itself to account for negative numbers.


Big O:
Big O is a notation used to describe the "computational complexity" of an algorithm. 
The computational complexity of an algorithm is split into two parts: time complexity and space complexity. 
- The time complexity of an algorithm is the amount of time the algorithm needs to run relative to the input size. 
- The space complexity of an algorithm is the amount of memory used by the algorithm relative to the input size.

When written, the function is wrapped by a capital O. Here are some example complexities:
- O(1): Constant
- O(n): Linear
- O(n^2): Quadratic
- O(2^n): Exponential
- O(logn): Logarithmic
- O(n * m)

Calculating Complexity:
Roughly, your function calculates the number of operations or amount of memory (depending on if you're analyzing time or space complexity, respectively) your algorithm consumes relative to the input size. 
Using the example from above (find the largest number in nums), we have a time complexity of O(n). 
The algorithm involves iterating over each element in nums, so if we define n as the length of nums, our algorithm uses approximately n steps. 
If we pass an array with a length of 10, it will perform approximately 10 steps. If we pass an array with a length of 10,000,000,000, it will perform approximately 10,000,000,000 steps.

Rules:
There are a few rules when it comes to calculating complexity. First, we ignore constants. That means 
O(9999999n)=O(8n)=O(n)=O( n / 500 ). Why do we do this? Imagine you had two algorithms. Algorithm A uses approximately n operations and 
algorithm B uses approximately 5n operations.

When n=100, algorithm A uses 100 operations and algorithm B uses 500 operations. What happens if we double n? 
Then algorithm A uses 200 operations and algorithm B uses 1000 operations. 
As you can see, when we double the value of n, both algorithms require double the amount of operations. 
If we were to 10x the value of n, then both algorithms would require 10x more operations.

Remember: the point of complexity is to analyze the algorithm as the input changes. 
We don't care that algorithm B is 5x slower than algorithm A. For both algorithms, as the input size increases, the number of operations required increases linearly. 
That's what we care about. Thus, both algorithms are O(n).

The second rule is that we consider the complexity as the variables tend to infinity. 
When we have addition/subtraction between terms of the same variable, we ignore all terms except the most powerful one. For example, O(2^n +n ^2 − 500n) = O(2^n). Why? 
Because as n tends to infinity, 2^n becomes so large that the other two terms are effectively zero in comparison.

Let's say that we had an algorithm that required n + 500 operations. 
It has a time complexity of O(n). When n is small, let's say n = 5, the +500 term is very significant - but we don't care about that. 
We need to perform the analysis as if n is tending toward infinity, and in that scenario, the 500 is nothing.

When talking about complexity, there are normally three cases:
- Best case scenario
- Average case
- Worst case scenario
In most algorithms, all three of these will be equal, but some algorithms will have them differ. 
If you have to choose only one to represent the algorithm's time or space complexity, never choose the best case scenario. 
It is most correct to use the worst case scenario, but you should be able to talk about the difference between the cases.


Analyzing Time Complexity:
Let's look at some example algorithms in pseudo-code and talk about their time complexities.

    // Given an integer array "arr" with length n,

    for (int num: arr) {
        print(num)
    }

This algorith has a time complexity of O(n). 
In each loop iteratio, we are performing a print, which costs O(1). The loop iterates n times, which gives a time complexity of (1 * n) = O(n)

    // Given an integer array "arr" with length n,

    for (int num: arr) {
        for (int i = 0; i < 500,000; i++) {
            print(num)
        }
    }

This algorithm has a time complexity of O(n).  In each inner for loop iteration, we are performing a print, which costs O(1).
This loop iterates 500,000 times, which means each outer loop iteration costs O(500000) = O(1). 
The outer for loop iterates n times, which gives a time complexity of O(n).

    // Given an integer array "arr" with length n,

    for (int num: arr) {
        for (int num2: arr) {
            print(num * num2)
        }
    }

This algorithm has a time complexity of O(n^2). The inner loop runs n times, which means each outer for loop iteration costs O(n).
The outer loop runs O(n) times, which gives a time complexity of O(n * n) = O(n^2).

    // Given integer arrays "arr" with length n and "arr2" with length m,

    for (int num: arr) {
        print(num)
    }

    or (int num: arr) {
        print(num)
    }

    for (int num: arr2) {
        print(num)
    }

This algorithm has a time complexity of O(n + m). The first two loops cost O(n), whereas the final loop costs O(m).
This gives a time complexity of O(2n + m) = O(n + m).

Logarithmic time:
A logarithm is the inverse operation to exponents. The time complexity O(logn) is called logarithmic time and is extremely fast. 
A common time complexity is O(n⋅logn), which is reasonably fast for most problems and also the time complexity of efficient sorting algorithms.

Typically, the base of the logarithm will be 2. This means that if your input is size n, then the algorithm will perform x operations, where 2^x = n. 
However, the base of the logarithm doesn't actually matter for big O, since all logarithms are related by a constant factor.

O(logn) means that somewhere in your algorithm, the input is being reduced by a percentage at every step. 
A good example of this is binary search, which is a searching algorithm that runs in O(logn) time (there is a chapter dedicated to binary search later on). 
With binary search, we initially consider the entire input (n elements). After the first step, we only consider n / 2 elements. 
After the second step, we only consider n / 4 elements, and so on. 
At each step, we are reducing our search space by 50%, which gives us a logarithmic time complexity.


Analyzing Space Complexity:
When you initialize variables like arrays or strings, your algorithm is allocating memory. 
We never count the space used by the input (it is bad practice to modify the input), and usually don't count the space used by the output (the answer) unless an interviewer asks us to.

    // Given an integer array "arr" with length n

    for (int num: arr) {
        print(num)
    }

Space complexity for this algorithm is O(1). The only space allocated is an integer variable num, which is constant relative to n.

    // Given an integer array "arr" with length n

    Array doubledNums = int[]

    for (int num: arr) {
        doubledNums.add(num * 2)
    }

This algorithm has Space complexity of O(n). The array doubledNums stores n integers at the end of the algorithm.

    // Given integer arrays "arr" with length n and "arr2" with length m,

    Array grid = int[n][m]

    for (int i = 0; i < arr.length; i++) {
        for (int j = 0; j < arr2.length; j++) {
            grid[i][j] = arr[i] * arr2[j]
        }
    }
This algorithm has a space complexity of O(n⋅m). We are creating a grid that has dimensions n⋅m.